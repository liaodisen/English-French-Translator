{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention in Transformers\n",
    "\n",
    "$Q$: what I am looking for: a matrix of $m \\times d$\n",
    "\n",
    "$K$: what I can offer: a matrix of $n \\times d$\n",
    "\n",
    "$V$: what I actually offer: a sequence of length $n \\times d$\n",
    "\n",
    "<img src=\"images/single_vector_attention.png\" width=\"400\">\n",
    "<img src=\"images/matrix_attention.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of attention, we assume the input has dimension 8, m = 4, n = 6\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "d, N, T = 8, 6, 4\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "Q = np.random.randn(T, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape:  (4, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Q shape: \", Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-attention\n",
    "$\\text{self attention} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "Here, divide by $\\sqrt{d_k}$ because for large values of $d_k$, the dot product grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43074867, -1.71528591, -3.59925887,  4.67274428, -2.21925453,\n",
       "         2.1634964 ],\n",
       "       [ 3.02294124,  5.7416805 , -1.12591825,  0.96355798, -3.16949911,\n",
       "        -1.27289228],\n",
       "       [-0.49062657, -0.39329731,  0.99936161, -4.67247317,  2.72622684,\n",
       "        -1.01645732],\n",
       "       [ 2.77303193,  1.60613201,  0.35052426, -2.81409016, -5.1375085 ,\n",
       "        -1.79713716]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Q, K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15229265, -0.60644515, -1.27253018,  1.65206458, -0.78462496,\n",
       "         0.76491149],\n",
       "       [ 1.06877112,  2.02999061, -0.39807222,  0.34066919, -1.12058716,\n",
       "        -0.45003538],\n",
       "       [-0.17346269, -0.1390516 ,  0.35332769, -1.65196873,  0.96386674,\n",
       "        -0.35937193],\n",
       "       [ 0.98041484,  0.56785342,  0.12392904, -0.99493112, -1.81638355,\n",
       "        -0.63538394]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(Q, K.T) / math.sqrt(d)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- Not required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.tril(np.ones((T, N)))\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15229265,        -inf,        -inf,        -inf,        -inf,\n",
       "               -inf],\n",
       "       [ 1.06877112,  2.02999061,        -inf,        -inf,        -inf,\n",
       "               -inf],\n",
       "       [-0.17346269, -0.1390516 ,  0.35332769,        -inf,        -inf,\n",
       "               -inf],\n",
       "       [ 0.98041484,  0.56785342,  0.12392904, -0.99493112,        -inf,\n",
       "               -inf]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.2766341 , 0.7233659 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.26820451, 0.27759435, 0.45420114, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.44937405, 0.29746429, 0.19082749, 0.06233416, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x)) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_Attention(q, k, v, mask=None):\n",
    "    d = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d)\n",
    "    if mask:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention\n",
    "\n",
    "value, attention = scaled_dot_product_Attention(Q, K, V, mask=None)\n",
    "print(value.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
